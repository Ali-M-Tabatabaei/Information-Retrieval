{"Title": "Towards More Practical Automation of Vulnerability Assessment", "Pages": null, "Cites in Papers": 0, "Cites in Patents": 0, "Full Text Views": 0, "Publisher": "IEEE", "DOI": null, "Date of Publication": "14-20 April 2024", "abstract": "It is increasingly suggested to identify emerging software vulner-abilities (SVs) through relevant development activities (e.g., issue reports) to allow early warnings to open source software (OSS) users. However, the support for the following assessment of the de-tected SVs has not yet been explored. SV assessment characterizes the detected SVs to prioritize limited remediation resources on the critical ones. To fill this gap, we aim to enable early vulnerability assessment based on SV-related issue reports (SIR). Besides, we observe the following concerns of the existing assessment techniques: 1) the assessment output lacks rationale and practical value; 2) the associations between Common Vulnerability Scoring System (CVSS) metrics have been ignored; 3) insufficient evaluation sce-narios and metrics. We address these concerns to enhance the prac-ticality of our proposed early vulnerability assessment approach (namely proEVA). Specifically, based on the observation of strong associations between CVSS metrics, we propose a prompt-based model to exploit such relations for CVSS metrics prediction. More-over, we design a curriculum-learning (CL) schedule to guide the model better learn such hidden associations during training. Aside from the standard classification metrics adopted in existing works, we propose two severity-aware metrics to provide a more compre-hensive evaluation regarding the prioritization of the high-severe SVs. Experimental results show that proEVA significantly outper-forms the baselines in both types of metrics. We further discuss the transferability of the prediction model regarding the upgrade of the assessment system, an important yet overlooked evaluation scenario in existing works. The results verify that proEVA is more efficient and flexible in migrating to different assessment systems.", "Published in": [{"name": "2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE)", "link": "https://ieeexplore.ieee.org/xpl/conhome/10548016/proceeding"}], "Authors": [{"name": "Shengyi Pan", "from": "The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, Zhejiang, China"}, {"name": "Lingfeng Bao", "from": "The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, Zhejiang, China"}, {"name": "Jiayuan Zhou", "from": "Centre for Software Excellence, Huawei, Kingston, Ontario, Canada"}, {"name": "Xing Hu", "from": "The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Ningbo, Zhejiang, China"}, {"name": "Xin Xia", "from": "Huawei, Hangzhou, Zhejiang, China"}, {"name": "Shanping Li", "from": "The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, Zhejiang, China"}], "IEEE keywords": ["Measurement", "Training", "Schedules", "Automation", "Predictive models", "Standards", "Open source software"], "Author Keywords": ["Software Security", "Vulnerability Assessment", "CVSS"]}